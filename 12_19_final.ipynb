{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_functions.final_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_train = MinMaxScaler()\n",
    "scaler_test = MinMaxScaler()\n",
    "\n",
    "# Hyper Parameter\n",
    "input_window = 100\n",
    "output_window = 60\n",
    "epochs = 150\n",
    "batch_size = 512\n",
    "\n",
    "# Dataset 불러오기\n",
    "df_DL = time_series_dataframe()\n",
    "df_DL_temp = df_DL['TEMP'].values\n",
    "df_DL_label = df_DL['label'].values\n",
    "\n",
    "train_len = int(len(df_DL) * 0.7)\n",
    "train_data = df_DL_temp[:train_len]\n",
    "test_data = df_DL_temp[train_len:]\n",
    "train_label = df_DL_label[:train_len]\n",
    "test_label = df_DL_label[train_len:]\n",
    "\n",
    "\n",
    "train_data = scaler_train.fit_transform(train_data.reshape(-1,1)).reshape(-1)\n",
    "test_data = scaler_test.fit_transform(test_data.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "train_data, _ = multistep_time_series(train_data, train_label, input_window, output_window)\n",
    "test_data, test_label = multistep_time_series(test_data, test_label, input_window, output_window)\n",
    "\n",
    "# X_train = train_data[:, 0, :].reshape((-1,input_window, 1))\n",
    "# y_train = train_data[:, 1, :].reshape((-1,input_window, 1))\n",
    "# X_test = test_data[:, 0, :].reshape((-1,input_window, 1))\n",
    "# y_test = test_data[:, 1, :].reshape((-1,input_window, 1))\n",
    "\n",
    "\n",
    "# PyTorch 돌리기위한 설정\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu' \n",
    "model = TransAm().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "# train_dataset = TimeSeiresDataset(X_train, y_train, input_window)\n",
    "# test_dataset = TimeSeiresDataset(X_test, y_test, input_window)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "train_data = train_data.to(device)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   181/  908 batches | lr 0.001000 | 55.19 ms | loss 0.02333 | ppl     1.02\n",
      "| epoch   1 |   362/  908 batches | lr 0.001000 | 52.90 ms | loss 0.00172 | ppl     1.00\n",
      "| epoch   1 |   543/  908 batches | lr 0.001000 | 52.70 ms | loss 0.00087 | ppl     1.00\n",
      "| epoch   1 |   724/  908 batches | lr 0.001000 | 52.80 ms | loss 0.00084 | ppl     1.00\n",
      "| epoch   1 |   905/  908 batches | lr 0.001000 | 53.35 ms | loss 0.00116 | ppl     1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 895.13s | valid loss 0.79595 | valid ppl     2.22 |\n",
      "---------------------------------------------------------------------------\n",
      "| epoch   2 |   181/  908 batches | lr 0.000960 | 54.55 ms | loss 0.03245 | ppl     1.03\n",
      "| epoch   2 |   362/  908 batches | lr 0.000960 | 52.81 ms | loss 0.00166 | ppl     1.00\n",
      "| epoch   2 |   543/  908 batches | lr 0.000960 | 52.63 ms | loss 0.00107 | ppl     1.00\n",
      "| epoch   2 |   724/  908 batches | lr 0.000960 | 52.80 ms | loss 0.00078 | ppl     1.00\n",
      "| epoch   2 |   905/  908 batches | lr 0.000960 | 52.90 ms | loss 0.00122 | ppl     1.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m train_tmp(model, train_data,batch_size, optimizer, criterion, input_window, output_window, epoch, scheduler)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     truth, test_result, result_to_ML, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mplot_and_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate2(model, test_data,criterion, input_window,output_window, input_window)\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\final_functions\\final_transformer.py:298\u001b[0m, in \u001b[0;36mplot_and_loss\u001b[1;34m(model, data_source, criterion, input_window, output_window, scaler_test)\u001b[0m\n\u001b[0;32m    296\u001b[0m data, target = get_batch(data_source, i,1, input_window)\n\u001b[0;32m    297\u001b[0m # look like the model returns static values for the output window\n\u001b[1;32m--> 298\u001b[0m output = model(data)\n\u001b[0;32m    299\u001b[0m if calculate_loss_over_all_values:\n\u001b[0;32m    300\u001b[0m     total_loss += criterion(output, target).item()\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\final_functions\\final_transformer.py:62\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m         src = self.pos_encoder(src)\n\u001b[1;32m---> 62\u001b[0m         output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n\u001b[0;32m     63\u001b[0m         output = self.decoder(output)\n\u001b[0;32m     64\u001b[0m         return output\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:707\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    705\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    708\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\functional.py:5440\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5437\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   5438\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 5440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5441\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   5443\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    train_tmp(model, train_data,batch_size, optimizer, criterion, input_window, output_window, epoch, scheduler)\n",
    "    \n",
    "    if (epoch % 1 == 0):\n",
    "        truth, test_result, result_to_ML, val_loss = plot_and_loss(model, test_data, criterion,input_window, output_window, scaler_test)\n",
    "    else:\n",
    "        val_loss = evaluate2(model, test_data,criterion, input_window,output_window, input_window)\n",
    "    \n",
    "    print('-' * 75)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f} |'.format(epoch, (time.time() - start_time),\n",
    "                                        val_loss, math.exp(val_loss)))\n",
    "    print('-' * 75)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_functions.final_ML import time_series_dataframe_ML\n",
    "\n",
    "df_ts_ML, _, _ = time_series_dataframe_ML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_functions.final_ML import ML, make_dataframe\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df_ML = make_dataframe(60,1)\n",
    "X = df_ML.iloc[:, :9].values\n",
    "y = df_ML['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { 'n_estimators' : [10,20,50,100],\n",
    "           'max_depth' : [10,20,30,40,50,60],\n",
    "           'min_samples_leaf' : [8, 12, 18],\n",
    "           'min_samples_split' : [8, 16, 20]\n",
    "            }\n",
    "\n",
    "# RandomForestClassifier 객체 생성 후 GridSearchCV 수행\n",
    "rf_clf = RandomForestClassifier(random_state = 42, n_jobs = -1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 3, n_jobs = -1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_model, ML_accuracy = ML()\n",
    "print(ML_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer output to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "columns = {'MEAN_TEMP' : [], 'STD' : [], 'MIN' : [], 'MAX' : [], 'SKEW' : [], 'KURT' : [], 'MEDIAN':[], '25%' : [], '75%' : []} #, 'label' : []\n",
    "for LIST in result_to_ML:\n",
    "    \n",
    "    MEAN = np.round(np.mean(LIST), 3)\n",
    "    MIN = np.min(LIST)\n",
    "    MAX = np.max(LIST)\n",
    "    STD = np.std(LIST)\n",
    "    median = np.median(LIST)\n",
    "    SKEW = skew(LIST)\n",
    "    KURT = kurtosis(LIST)\n",
    "    a, b = np.percentile(LIST, q = [25,75])\n",
    "    \n",
    "    #columns['label'].append(i)\n",
    "    columns['MEAN_TEMP'].append(MEAN)\n",
    "    columns['MIN'].append(MIN)\n",
    "    columns['MAX'].append(MAX)\n",
    "    columns['STD'].append(STD)\n",
    "    columns['SKEW'].append(SKEW)\n",
    "    columns['KURT'].append(KURT)\n",
    "    columns['MEDIAN'].append(np.round(median,3))\n",
    "    columns['25%'].append(np.round(a,3))\n",
    "    columns['75%'].append(np.round(b,3))\n",
    "\n",
    "LABEL = pd.DataFrame({'label' : test_label})\n",
    "\n",
    "dataframe = pd.DataFrame(columns)\n",
    "dataframe = pd.concat([dataframe, LABEL], axis = 1)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanhwa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
