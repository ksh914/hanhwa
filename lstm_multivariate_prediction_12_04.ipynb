{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'function_file' from 'c:\\\\Users\\\\Sejong\\\\Desktop\\\\hanhwa\\\\function_file\\\\__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import function_file as ff\n",
    "import importlib\n",
    "importlib.reload(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP</th>\n",
       "      <th>label</th>\n",
       "      <th>TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>286.797</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287.082</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>285.938</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>285.772</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286.357</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664791</th>\n",
       "      <td>865.029</td>\n",
       "      <td>11</td>\n",
       "      <td>664791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664792</th>\n",
       "      <td>864.985</td>\n",
       "      <td>11</td>\n",
       "      <td>664792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664793</th>\n",
       "      <td>865.048</td>\n",
       "      <td>11</td>\n",
       "      <td>664793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664794</th>\n",
       "      <td>865.040</td>\n",
       "      <td>11</td>\n",
       "      <td>664794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664795</th>\n",
       "      <td>864.972</td>\n",
       "      <td>11</td>\n",
       "      <td>664795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>664796 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           TEMP  label    TIME\n",
       "0       286.797      1       0\n",
       "1       287.082      1       1\n",
       "2       285.938      1       2\n",
       "3       285.772      1       3\n",
       "4       286.357      1       4\n",
       "...         ...    ...     ...\n",
       "664791  865.029     11  664791\n",
       "664792  864.985     11  664792\n",
       "664793  865.048     11  664793\n",
       "664794  865.040     11  664794\n",
       "664795  864.972     11  664795\n",
       "\n",
       "[664796 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from function_file.time_series import time_series_dataframe\n",
    "data = time_series_dataframe()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as dc\n",
    "def prepare_dataframe_for_lstm(df, lookback_size,output_size):\n",
    "    df = dc(df)\n",
    "    \n",
    "    for i in range(1,lookback_size+1):\n",
    "        df[f'TEMP(t-{i})'] = df['TEMP'].shift(i)\n",
    "\n",
    "    for j in range(1,output_size + 1):\n",
    "        df[f'TEMP(t+{j})'] = df['TEMP'].shift(-j)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def multi_step_LSTM(df, lookback_size, output_size, batch_size):\n",
    "    shifted_df = prepare_dataframe_for_lstm(df, lookback_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_dataframe_for_lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\lstm_multivariate_prediction_12_04.ipynb 셀 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mTEMP\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(df) \u001b[39m*\u001b[39m \u001b[39m0.8\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m shifted_df \u001b[39m=\u001b[39m prepare_dataframe_for_lstm(data, \u001b[39m30\u001b[39m,\u001b[39m9\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m shifted_df\u001b[39m.\u001b[39mcolumns\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prepare_dataframe_for_lstm' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = data['TEMP']\n",
    "train_len = int(len(df) * 0.8)\n",
    "\n",
    "shifted_df = prepare_dataframe_for_lstm(data, 30,9)\n",
    "shifted_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "window_size = 30\n",
    "output_size = 10\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]    \n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(window_size,1)) # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(window_size,1))\n",
    "    return input, target\n",
    "\n",
    "import math\n",
    "import time\n",
    "model = LSTM(1,4,1).to(device)\n",
    "batch_size = 512\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = lr)\n",
    "criterion = nn.MSELoss()\n",
    "def train(train_data):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, target = get_batch(train_data, i, batch_size)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output[-output_size:], target[-output_size:])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot\n",
    "window_size = 30\n",
    "output_size = 10\n",
    "df = data['TEMP'].values\n",
    "def create_inout_sequences(input_data, window_size):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-window_size):\n",
    "        train_seq = np.append(input_data[i:i+window_size][:-output_size] , output_size * [0])\n",
    "        train_label = input_data[i:i+window_size]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "\n",
    "def plot_and_loss(eval_model, data_source):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)\n",
    "            \n",
    "            total_loss += criterion(output[-output_size:], target[-output_size:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-output_size:].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-output_size:].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "\n",
    "    test_result = scaler_test.inverse_transform(test_result.reshape(-1,1)).reshape(-1)\n",
    "    truth = scaler_test.inverse_transform(truth.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "    pyplot.plot(test_result,color=\"red\")\n",
    "    pyplot.plot(truth,color=\"blue\")\n",
    "    #pyplot.plot(test_result-truth,color=\"green\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.ylim([600,900])\n",
    "    pyplot.show()\n",
    "    pyplot.close()\n",
    "\n",
    "    return test_result, truth, total_loss / i\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 512\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            total_loss += len(data[0])* criterion(output[-output_size:], targets[-output_size:]).cpu().item()            \n",
    "    return total_loss / len(data_source)\n",
    "\n",
    "train_len = int(len(df) * 0.8)\n",
    "train_data = df[:train_len]\n",
    "test_data = df[train_len:]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_train = MinMaxScaler()\n",
    "scaler_test = MinMaxScaler()\n",
    "train_data = scaler_train.fit_transform(train_data.reshape(-1,1)).reshape(-1)\n",
    "test_data = scaler_test.fit_transform(test_data.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "\n",
    "train_sequence = create_inout_sequences(train_data,window_size)\n",
    "test_data = create_inout_sequences(test_data,window_size)\n",
    "train_sequence = train_sequence[:-output_size]\n",
    "test_data = test_data[:-output_size]\n",
    "\n",
    "train_sequence = train_sequence.to(device)\n",
    "test_data = test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   207/ 1038 batches | lr 0.001000 |  5.38 ms | loss 0.00368 | ppl     1.00\n",
      "| epoch   1 |   414/ 1038 batches | lr 0.001000 |  4.99 ms | loss 0.00138 | ppl     1.00\n",
      "| epoch   1 |   621/ 1038 batches | lr 0.001000 |  4.78 ms | loss 0.00148 | ppl     1.00\n",
      "| epoch   1 |   828/ 1038 batches | lr 0.001000 |  4.90 ms | loss 0.00221 | ppl     1.00\n",
      "| epoch   1 |  1035/ 1038 batches | lr 0.001000 |  6.37 ms | loss 0.00215 | ppl     1.00\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  6.38s | valid loss 0.24553 | valid ppl     1.28 |\n",
      "------------------------------------------------------------------------------------------\n",
      "| epoch   2 |   207/ 1038 batches | lr 0.001000 |  4.97 ms | loss 0.07748 | ppl     1.08\n",
      "| epoch   2 |   414/ 1038 batches | lr 0.001000 |  5.10 ms | loss 0.00153 | ppl     1.00\n",
      "| epoch   2 |   621/ 1038 batches | lr 0.001000 |  4.92 ms | loss 0.00141 | ppl     1.00\n",
      "| epoch   2 |   828/ 1038 batches | lr 0.001000 |  4.99 ms | loss 0.00195 | ppl     1.00\n",
      "| epoch   2 |  1035/ 1038 batches | lr 0.001000 |  4.99 ms | loss 0.00176 | ppl     1.00\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  6.09s | valid loss 0.25022 | valid ppl     1.28 |\n",
      "------------------------------------------------------------------------------------------\n",
      "| epoch   3 |   207/ 1038 batches | lr 0.001000 |  5.27 ms | loss 0.09268 | ppl     1.10\n",
      "| epoch   3 |   414/ 1038 batches | lr 0.001000 |  4.95 ms | loss 0.00168 | ppl     1.00\n",
      "| epoch   3 |   621/ 1038 batches | lr 0.001000 |  4.84 ms | loss 0.00148 | ppl     1.00\n",
      "| epoch   3 |   828/ 1038 batches | lr 0.001000 |  4.99 ms | loss 0.00200 | ppl     1.00\n",
      "| epoch   3 |  1035/ 1038 batches | lr 0.001000 |  4.92 ms | loss 0.00161 | ppl     1.00\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  6.09s | valid loss 0.25279 | valid ppl     1.29 |\n",
      "------------------------------------------------------------------------------------------\n",
      "| epoch   4 |   207/ 1038 batches | lr 0.001000 |  6.43 ms | loss 0.10863 | ppl     1.11\n",
      "| epoch   4 |   414/ 1038 batches | lr 0.001000 |  5.19 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch   4 |   621/ 1038 batches | lr 0.001000 |  4.89 ms | loss 0.00160 | ppl     1.00\n",
      "| epoch   4 |   828/ 1038 batches | lr 0.001000 |  5.08 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch   4 |  1035/ 1038 batches | lr 0.001000 |  5.00 ms | loss 0.00155 | ppl     1.00\n",
      "------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  6.42s | valid loss 0.25337 | valid ppl     1.29 |\n",
      "------------------------------------------------------------------------------------------\n",
      "| epoch   5 |   207/ 1038 batches | lr 0.001000 |  5.45 ms | loss 0.11880 | ppl     1.13\n",
      "| epoch   5 |   414/ 1038 batches | lr 0.001000 |  5.04 ms | loss 0.00191 | ppl     1.00\n",
      "| epoch   5 |   621/ 1038 batches | lr 0.001000 |  4.99 ms | loss 0.00171 | ppl     1.00\n",
      "| epoch   5 |   828/ 1038 batches | lr 0.001000 |  5.21 ms | loss 0.00216 | ppl     1.00\n",
      "| epoch   5 |  1035/ 1038 batches | lr 0.001000 |  5.01 ms | loss 0.00157 | ppl     1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\lstm_multivariate_prediction_12_04.ipynb 셀 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train(train_sequence)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m (epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     test_result, truth, val_loss \u001b[39m=\u001b[39m plot_and_loss(model, test_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(model, test_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "for epoch in range(1,epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_sequence)\n",
    "\n",
    "    if (epoch % 5 == 0):\n",
    "        test_result, truth, val_loss = plot_and_loss(model, test_data)\n",
    "\n",
    "    else:\n",
    "        val_loss = evaluate(model, test_data)\n",
    "\n",
    "    print('-' * 90)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f} |'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([531806, 2, 30])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  6 10:56:37 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.58                 Driver Version: 537.58       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080      WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   41C    P8              39W / 340W |   1323MiB / 10240MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4908    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10644    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     10712    C+G   ...0_x64__8wekyb3d8bbwe\\HxAccounts.exe    N/A      |\n",
      "|    0   N/A  N/A     10740    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10796    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     12104    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     15048    C+G   ...on\\119.0.2151.97\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     15116    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15200    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     17796    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     18752    C+G   ...ta\\Local\\Programs\\Notion\\Notion.exe    N/A      |\n",
      "|    0   N/A  N/A     20988    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     21072    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21972    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     22800    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([286.7970, 287.0820, 285.9380, 285.7720, 286.3570, 286.2770, 285.3190,\n",
       "        285.0250, 284.9260, 285.1400, 285.5990, 285.8470, 286.6900, 286.4550,\n",
       "        286.5330, 286.3730, 286.3090, 285.8390, 284.9660, 284.4830,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "y = pd.concat([shifted_df.iloc[:, 0], shifted_df.iloc[:, 31:]], axis = 1)\n",
    "X = shifted_df.iloc[:, 1:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((664757, 30), (664757, 10))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((531805, 30), (531805, 10), (132952, 30), (132952, 10))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = int(len(X) * 0.8)\n",
    "X_train = X[:train_len]\n",
    "y_train = y[:train_len]\n",
    "\n",
    "X_test = X[train_len:]\n",
    "y_test = y[train_len:]\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "lookback_size = 30\n",
    "output_size = 10\n",
    "X_train = X_train.reshape((-1,lookback_size,1))\n",
    "X_test = X_test.reshape((-1,lookback_size, 1))\n",
    "y_train = y_train.reshape((-1, output_size, 1))\n",
    "y_test = y_test.reshape((-1, output_size, 1))\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "y_test = torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([531805, 30, 1]),\n",
       " torch.Size([531805, 10, 1]),\n",
       " torch.Size([132952, 30, 1]),\n",
       " torch.Size([132952, 10, 1]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "import torch.nn as nn\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        #out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(1,4,1).to(device)\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = lr)\n",
    "criterion = nn.MSELoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        training_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    A, B = batch[0].to(device), batch[1].to(device)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 30, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 30, 1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_stacked_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "print(A.shape)\n",
    "model = LSTM(1,4,1).to(device)\n",
    "output = model(A)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([512, 10, 1])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\lstm_multivariate_prediction_12_04.ipynb 셀 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train()\n",
      "\u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\lstm_multivariate_prediction_12_04.ipynb 셀 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X_batch, y_batch \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m output \u001b[39m=\u001b[39m model(X_batch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m training_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sejong/Desktop/hanhwa/lstm_multivariate_prediction_12_04.ipynb#X41sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\nn\\functional.py:3328\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3325\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3326\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3328\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3329\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\Sejong\\Desktop\\hanhwa\\hanhwa\\Lib\\site-packages\\torch\\functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for i in range(10):\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEViCES'] =\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanhwa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
